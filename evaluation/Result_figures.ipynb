{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0   1   2   3   4  5  6  7  8  9  10  11  12\n",
      "0    0   0   0   0   0  0  0  0  0  0   0   0   0\n",
      "1    0  20   0   0   0  0  0  0  0  0   0   0   0\n",
      "2    0   1  19   0   0  0  0  0  0  0   0   0   0\n",
      "3    0   1  19   0   0  0  0  0  0  0   0   0   0\n",
      "4    0   0   3  17   0  0  0  0  0  0   0   0   0\n",
      "..  ..  ..  ..  ..  .. .. .. .. .. ..  ..  ..  ..\n",
      "536  0   0  19   0   0  0  0  0  0  0   0   0   0\n",
      "537  0   0   3   1  14  1  0  0  0  0   0   0   0\n",
      "538  0   0   4  15   0  0  0  0  0  0   0   0   0\n",
      "539  3  16   0   0   0  0  0  0  0  0   0   0   0\n",
      "540  0   0   2   4  13  0  0  0  0  0   0   0   0\n",
      "\n",
      "[541 rows x 13 columns]\n",
      "     0  1  2  3  4  5  6  7  8  9  10  11  12\n",
      "0    0  0  0  0  0  0  0  0  0  0   0   0   0\n",
      "1    0  0  0  0  0  0  0  0  0  0   0   0   0\n",
      "2    0  0  0  0  0  0  0  0  0  0   0   0   0\n",
      "3    0  0  0  0  0  0  0  0  0  0   0   0   0\n",
      "4    0  0  0  0  0  0  0  0  0  0   0   0   0\n",
      "..  .. .. .. .. .. .. .. .. .. ..  ..  ..  ..\n",
      "536  0  0  1  0  0  0  0  0  0  0   0   0   0\n",
      "537  0  0  0  0  1  0  0  0  0  0   0   0   0\n",
      "538  0  0  0  1  0  0  0  0  0  0   0   0   0\n",
      "539  1  0  0  0  0  0  0  0  0  0   0   0   0\n",
      "540  0  0  1  0  0  0  0  0  0  0   0   0   0\n",
      "\n",
      "[541 rows x 13 columns]\n",
      "     0   1   2   3   4  5  6  7  8  9  10  11  12\n",
      "0    0   0   0   0   0  0  0  0  0  0   0   0   0\n",
      "1    0  20   0   0   0  0  0  0  0  0   0   0   0\n",
      "2    0   1  19   0   0  0  0  0  0  0   0   0   0\n",
      "3    0   1  19   0   0  0  0  0  0  0   0   0   0\n",
      "4    0   0   3  17   0  0  0  0  0  0   0   0   0\n",
      "..  ..  ..  ..  ..  .. .. .. .. .. ..  ..  ..  ..\n",
      "536  0   0  20   0   0  0  0  0  0  0   0   0   0\n",
      "537  0   0   3   1  15  1  0  0  0  0   0   0   0\n",
      "538  0   0   4  16   0  0  0  0  0  0   0   0   0\n",
      "539  4  16   0   0   0  0  0  0  0  0   0   0   0\n",
      "540  0   0   3   4  13  0  0  0  0  0   0   0   0\n",
      "\n",
      "[541 rows x 13 columns]\n"
     ]
    }
   ],
   "execution_count": 3,
   "source": [
    "# Extract final score\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "auto_data = pd.read_csv(r\"F:\\Projects\\LLMValueInvestigation\\code\\human_evaluation\\outputs\\Mixtral-8x7B-Instruct-v0.1-bnb-4bit\\Chinese\\mistral\\qa_score.csv\", index_col=0)\n",
    "\n",
    "print(auto_data)\n",
    "\n",
    "def findAllFile(base):\n",
    "    for root, ds, fs in os.walk(base):\n",
    "        for f in fs:\n",
    "            if f.endswith(\"scores.csv\"):\n",
    "                fullname = os.path.join(root, f)\n",
    "                yield fullname\n",
    "            \n",
    "base = \"F:\\\\Projects\\\\LLMValueInvestigation\\\\code\\\\human_evaluation\\\\human_eval\\\\Mixtral-8x7B-Instruct-v0.1-bnb-4bit\\\\Chinese\\\\mistral\\\\\"\n",
    "human_data = pd.read_csv(base + \"extra_score.csv\", index_col=0).fillna(0)\n",
    "print(human_data)\n",
    "auto_data += human_data\n",
    "\n",
    "print(auto_data)\n",
    "\n",
    "auto_data.to_csv(base+\"final_score.csv\", index=True)"
   ],
   "id": "9117be44-62d0-4a62-92f9-8012e3583d1b"
  },
  {
   "cell_type": "code",
   "id": "d52381f8-2dfe-4c42-a20b-a08095e28a2f",
   "metadata": {},
   "source": [
    "# Calculate KLD\n",
    "import scipy\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import math\n",
    "base = \"F:\\\\Projects\\\\LLMValueInvestigation\\\\code\\\\human_evaluation\\\\human_eval\\\\Mixtral-8x7B-Instruct-v0.1-bnb-4bit\\\\Chinese\\\\mistral\\\\\"\n",
    "\n",
    "final_data = pd.read_csv(base+\"final_score.csv\", index_col=0).fillna(0)[1:]\n",
    "p_model_list = []\n",
    "\n",
    "for i in final_data.index:\n",
    "    if i != 0:\n",
    "        line = final_data.loc[i]\n",
    "        # print(line)\n",
    "        prob_line = line / line.sum()\n",
    "        p_model_list.append(prob_line)\n",
    "# print(p_model_list)\n",
    "# D:\\projects\\LLMValueInvestigation\\code\\human_evaluation\\survey_data\\AECE_v1\\preferences\n",
    "ref_data = pd.read_csv(r\"F:\\Projects\\LLMValueInvestigation\\code\\human_evaluation\\survey_data\\AECE_v1\\preferences\\zh_ref_score_all.csv\", index_col=0)\n",
    "ref_data = ref_data.fillna(0)\n",
    "    \n",
    "question_type = pd.read_excel(\n",
    "    r\"F:\\Projects\\LLMValueInvestigation\\code\\human_evaluation\\survey_data\\AECE_v1\\Originals\\CN_survey\\result_database\\Question_type.xlsx\"\n",
    ")\n",
    "types = question_type[\"value type\"]\n",
    "kl_list = []\n",
    "ques_num = []\n",
    "p_models= []\n",
    "for i in range(len(p_model_list)):\n",
    "    p_model = p_model_list[i]/p_model_list[i].sum()\n",
    "    p_ref = ref_data.loc[i+1]/ref_data.loc[i+1].sum()\n",
    "    kl_i = scipy.stats.entropy(p_model, p_ref)\n",
    "    # # print(p_model)\n",
    "    # print(p_model_list[i])\n",
    "    # # print(p_ref)\n",
    "    # print(ref_data.loc[i+1])\n",
    "    # print(kl_i)\n",
    "    kl_list.append(kl_i)\n",
    "    p_models.append(p_model)\n",
    "    ques_num.append(i+1)\n",
    "\n",
    "\n",
    "kl_data = pd.DataFrame({\"question Number\": ques_num, \"kl divergence\": kl_list,\n",
    "                        \"type\": types[:len(ques_num)]\n",
    "                       })\n",
    "dmax = 0\n",
    "close_count = 0\n",
    "for i in range(len(kl_data)):\n",
    "    if kl_data.loc[i][\"kl divergence\"] < 0.5:\n",
    "        close_count += 1\n",
    "    if kl_data.loc[i][\"kl divergence\"] == numpy.inf:\n",
    "        continue\n",
    "    elif kl_data.loc[i][\"kl divergence\"] > dmax:\n",
    "        dmax = kl_data.loc[i][\"kl divergence\"]\n",
    "if numpy.nan in kl_data:\n",
    "    print(a)\n",
    "print(close_count)\n",
    "print(dmax)\n",
    "kl_data = kl_data.replace(numpy.inf, dmax+1)\n",
    "kl_data = kl_data.replace(numpy.nan, dmax+1)\n",
    "print(f\"Mean: {numpy.mean(kl_data['kl divergence'])}\")\n",
    "print(f\"STD: {numpy.mean(numpy.std(p_models, axis=1))}\")\n",
    "kl_data.to_csv(f\"{base}Mean-{numpy.mean(kl_data['kl divergence'])}-qbq_kld.csv\", index=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e5684d32-2b75-4e26-8fae-df8bf9d31b94",
   "metadata": {},
   "source": [
    "# Diversity\n",
    "qa_dict = {}\n",
    "for j in range(100):\n",
    "    qa_dict[j] = []\n",
    "for i in range(19):\n",
    "    with open(f\"D:\\\\projects\\\\LLMValueInvestigation\\\\code\\\\investigate\\\\outputs\\\\DEF-wo_ppt\\\\Chinese\\\\dolphin-2.2.1-mistral-7b\\\\{i}\\\\0_test_responses.txt\", \n",
    "              \"r\", encoding=\"utf-8\") as ans:\n",
    "        answers_raw = ans.read()\n",
    "        answers_raw = answers_raw.split(\"\\n\")\n",
    "        \n",
    "    answers = []\n",
    "    \n",
    "    for i in range(len(answers_raw)):\n",
    "        if answers_raw[i].startswith(\"Q\"):\n",
    "            answer = answers_raw[i]\n",
    "            answers.append(answer)\n",
    "    \n",
    "        elif answers_raw != \"\\n\":\n",
    "            answers[-1] += answers_raw[i]\n",
    "            \n",
    "    # assert len(answers) == len(choices_list), f\"length error: ans {len(answers)}, ques {len(choices_list)}\"\n",
    "    for j in range(100):\n",
    "        qa_dict[j].append(answers[j])\n",
    "\n",
    "un_count = 0\n",
    "for k in range(100):\n",
    "    qa_cleaned = list(set(qa_dict[k]))\n",
    "    un_count += len(qa_cleaned)\n",
    "print(un_count-100)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7aadaacd-f698-4b15-8aba-cc9564c42dc0",
   "metadata": {},
   "source": [
    "from math import log2\n",
    "# Case KL-D Single\n",
    "import scipy\n",
    "import numpy\n",
    "import pandas as pd\n",
    "base = \"F:\\\\Projects\\\\LLMValueInvestigation\\\\code\\\\human_evaluation\\\\outputs\\\\Mixtral-8x7B-Instruct-v0.1-bnb-4bit\\\\Chinese\\\\mistral\\\\\"\n",
    "ref_data = pd.read_csv(r\"F:\\Projects\\LLMValueInvestigation\\code\\human_evaluation\\survey_data\\AECE_v1\\preferences\\zh_ref_score_all.csv\", index_col=0)\n",
    "ref_data = ref_data.fillna(0)\n",
    "\n",
    "mean_list = []\n",
    "for i in range(20):\n",
    "    final_data = pd.read_csv(base + str(i)+\"\\\\qa_score_single.csv\", index_col=0)\n",
    "    p_model_list = []\n",
    "    for i in final_data.index:\n",
    "        if i != 0:\n",
    "            line = final_data.loc[i]\n",
    "            prob_line = line / line.sum()\n",
    "            p_model_list.append(prob_line)\n",
    "    # print(p_model_list)\n",
    "    \n",
    "    # calculate the kl divergence\n",
    "    def kl_divergence(p, q):\n",
    "     return sum(p[i] * log2(p[i]/q[i]) for i in range(len(p)))\n",
    "        \n",
    "    question_type = pd.read_excel(\n",
    "        r\"F:\\Projects\\LLMValueInvestigation\\code\\human_evaluation\\survey_data\\AECE_v1\\Originals\\CN_survey\\result_database\\Question_type.xlsx\"\n",
    "    )\n",
    "    types = question_type[\"value type\"]\n",
    "    kl_list = []\n",
    "    ques_num = []\n",
    "    for i in range(len(p_model_list)):\n",
    "        p_model = p_model_list[i]/p_model_list[i].sum()\n",
    "        p_ref = ref_data.loc[i+1]/ref_data.loc[i+1].sum()\n",
    "        kl_i = scipy.stats.entropy(p_model, p_ref)\n",
    "        # print(kl_i)\n",
    "        kl_list.append(kl_i)\n",
    "        ques_num.append(i+1)\n",
    "    \n",
    "    \n",
    "    kl_data = pd.DataFrame({\"question Number\": ques_num, \"kl divergence\": kl_list, \"type\": types})\n",
    "    dmax = 0\n",
    "    close_count = 0\n",
    "    for i in range(len(kl_data)):\n",
    "        if kl_data.loc[i][\"kl divergence\"] < 0.5:\n",
    "            close_count += 1\n",
    "        if kl_data.loc[i][\"kl divergence\"] == numpy.inf:\n",
    "            continue\n",
    "        elif kl_data.loc[i][\"kl divergence\"] > dmax:\n",
    "            dmax = kl_data.loc[i][\"kl divergence\"]\n",
    "    if numpy.nan in kl_data:\n",
    "        print(a)\n",
    "    kl_data = kl_data.replace(numpy.inf, dmax+1)\n",
    "    kl_mean = kl_data[\"kl divergence\"].mean()\n",
    "    mean_list.append(kl_mean)\n",
    "print(sorted(range(len(mean_list)), key=lambda k: mean_list[k]))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7fa77caf-314d-48c1-8479-48a892f5fa8c",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Single Character\n",
    "model = \"Mixtral-8x7B-Instruct-v0.1-bnb-4bit\"\n",
    "model_short = \"mistral\"\n",
    "test_type = \"CN_survey\"\n",
    "lang = \"Chinese\"\n",
    "lang_short = \"zh\"\n",
    "target = \"5\"\n",
    "\n",
    "data_base = \"F:\\\\Projects\\\\LLMValueInvestigation\\\\code\\\\human_evaluation\\\\survey_data\\\\AECE_v1\\\\preferences\\\\\"\n",
    "var_chara_data = pd.read_excel(rf\"F:\\Projects\\LLMValueInvestigation\\code\\human_evaluation\\survey_data\\AECE_v1\\Originals\\{test_type}\\Character.xlsx\", index_col=[0, 1])\n",
    "base = f\"F:\\\\Projects\\\\LLMValueInvestigation\\\\code\\\\human_evaluation\\\\outputs\\\\{model}\\\\{lang}\\\\{model_short}\"\n",
    "human_res_data = pd.read_csv(f\"{data_base}{lang_short}_ref_score_all.csv\")[1:].drop('12', axis=1)\n",
    "model_res_data = pd.read_csv(f\"{base}\\\\{target}\\\\qa_score_single.csv\", index_col=0)[1:].drop('12', axis=1)\n",
    "\n",
    "profile_data = pd.DataFrame(columns=[\"Match\", \"Sex\", \"Age\"], index=range(1,614))\n",
    "multiple_choice_ques = [7, 8, 227, 228]\n",
    "\n",
    "for i in range(1, len(model_res_data) + 1):\n",
    "    # print(i)\n",
    "    model_prefer = model_res_data.loc[i].idxmax()\n",
    "    human_prefer = human_res_data.loc[i].idxmax()\n",
    "    if model_prefer == human_prefer:\n",
    "        profile_data.loc[i][\"Match\"] = \"Y\"\n",
    "    else:\n",
    "        profile_data.loc[i][\"Match\"] = \"N\"\n",
    "    if i not in multiple_choice_ques:\n",
    "        # print(var_chara_data.index)\n",
    "        if (str(i), float(model_prefer)) in var_chara_data.index:\n",
    "            profile_data.loc[i][\"Sex\"] = var_chara_data.loc[str(i), float(model_prefer)][\"Sex\"]\n",
    "            profile_data.loc[i][\"Age\"] = var_chara_data.loc[str(i), int(model_prefer)][\"Age\"]\n",
    "        else:\n",
    "            profile_data.loc[i][\"Sex\"] = \"Null\"\n",
    "            profile_data.loc[i][\"Age\"] = \"Null\"\n",
    "    else:\n",
    "        if (f\"{str(i)}-{int(model_prefer)}\", 1.0) in var_chara_data.index:\n",
    "            profile_data.loc[i][\"Sex\"] = var_chara_data.loc[f\"{str(i)}-{int(model_prefer)}\", 1.0][\"Sex\"]\n",
    "            profile_data.loc[i][\"Age\"] = var_chara_data.loc[f\"{str(i)}-{int(model_prefer)}\", 1.0][\"Age\"]\n",
    "        else:\n",
    "            profile_data.loc[i][\"Sex\"] = \"Null\"\n",
    "            profile_data.loc[i][\"Age\"] = \"Null\"\n",
    "        # print(profile_data.loc[i])\n",
    "chara_base = r\"F:\\Projects\\LLMValueInvestigation\\code\\human_evaluation\\survey_data\\AECE_v1\\Originals\"\n",
    "question_type = pd.read_excel(\n",
    "    f\"{chara_base}\\\\{test_type}\\\\result_database\\\\Question_type.xlsx\"\n",
    ")\n",
    "types = question_type[\"value type\"]\n",
    "types.index = range(1, len(types) + 1)\n",
    "profile_data = pd.concat([profile_data, types], axis=1,)\n",
    "# profile_data\n",
    "profile_data.to_excel(f\"{base}\\\\{target}\\\\Character.xlsx\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b7fe7f79-8ecf-4a40-9010-40f6e35c696e",
   "metadata": {},
   "source": [
    "# QBQ KL-D\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "sns.set_context('paper')\n",
    "\n",
    "# model_names = [\"baichuan2-13B-4bits\",\"WizardLM-13B-V1.2\",\"ChatGLM2-6B\", \"dolphin-2.2.1-mistral-7b\", \"Meta-Llama-3-8B-Instruct\", \"Llama3-Chinese-8B-Instruct\"]\n",
    "model_name = \"Mixtral-8x7B-Instruct-v0.1-bnb-4bit\"\n",
    "cn_base = \"F:\\\\Projects\\\\LLMValueInvestigation\\\\code\\\\human_evaluation\\\\human_eval\\\\CN_survey_all\\\\\" + model_name + \"\\\\\"\n",
    "us_base = \"F:\\\\Projects\\\\LLMValueInvestigation\\\\code\\\\human_evaluation\\\\human_eval\\\\US_survey_all\\\\\" + model_name + \"\\\\\"\n",
    "cn_kld_data = pd.read_csv(cn_base+\"qbq_kld.csv\", index_col=0)\n",
    "us_kld_data = pd.read_csv(us_base+\"qbq_kld.csv\", index_col=0)\n",
    "\n",
    "cn_test_type = pd.DataFrame([\"CN Survey\"]*len(cn_kld_data), index=range(len(cn_kld_data)), columns=[\"Test type\"])\n",
    "us_test_type = pd.DataFrame([\"US Survey\"]*len(us_kld_data), index=range(len(us_kld_data)), columns=[\"Test type\"])\n",
    "\n",
    "cn_kld = pd.concat([cn_kld_data, cn_test_type], axis=1)\n",
    "us_kld = pd.concat([us_kld_data, us_test_type], axis=1)\n",
    "kl_data = pd.concat([us_kld, cn_kld], axis=0, ignore_index=True)\n",
    "kl_data.columns = [\"Num question\", \"KL-Div\", \"Value type\", \"Test type\"]\n",
    "print(kl_data)\n",
    "ax = sns.violinplot(data=kl_data, x=\"Value type\", y=\"KL-Div\", hue=\"Test type\", split=True, palette=\"coolwarm\")\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "barplt = ax.get_figure()\n",
    "barplt.savefig(f\"{model_name}-qbq.pdf\",  bbox_inches='tight')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fbc876d2-62e8-4ccb-b881-c3cb54437191",
   "metadata": {},
   "source": [
    "# Value type KL-D\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "model_name = [\"Baichuan2-13B-Chat\",\"WizardLM-13B\",\"ChatGLM2-6B\", \"Mistral-7B-Instruct\",\"Dolphin-2.2.1-Mistral-7B\",\"Mixtral-8x7B-Instruct\", \"Llama-3-8B\",\n",
    "              \"Llama-3-8B-Instruct\",\"Dolphin-2.9.1-Llama-3-8B\", \"Llama-3-Chinese-8B-Instruct\", \"Claude-3.5-Sonnet\"]\n",
    "\n",
    "mean_data = pd.DataFrame()\n",
    "overall_mean = {}\n",
    "for i in range(len(model_name)):\n",
    "    name = model_name[i]\n",
    "    cn_base = \"F:\\\\Projects\\\\LLMValueInvestigation\\\\code\\\\human_evaluation\\\\human_eval\\\\CN_survey_all\\\\\" + name + \"\\\\\"\n",
    "    cn_kld_data = pd.read_csv(cn_base+\"qbq_kld.csv\", index_col=0)\n",
    "    cn_test_type = pd.DataFrame([\"CN Survey\"]*len(cn_kld_data), index=range(len(cn_kld_data)), columns=[\"Test type\"])\n",
    "    cn_kld = pd.concat([cn_kld_data, cn_test_type], axis=1)\n",
    "    cn_kld.columns = [\"Num question\", \"KL-Div\", \"Value type\", \"Test type\"]\n",
    "    \n",
    "    cn_means = cn_kld.groupby('Value type')['KL-Div'].mean()\n",
    "    mean = cn_means.mean()\n",
    "    overall_mean[name] = mean\n",
    "    temp = {\n",
    "        \"Value type\": cn_means.index,\n",
    "        \"Mean KL-D\": cn_means.values,\n",
    "        \"Model\": [name] * len(cn_means)\n",
    "    }\n",
    "    \n",
    "    mean_temp = pd.DataFrame(temp)\n",
    "    mean_data = pd.concat([mean_data, mean_temp], ignore_index=True)\n",
    "print(sorted(overall_mean.items(), key=lambda x: x[1], reverse=True))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fcb04ec1-3f47-4843-91e8-48952b459b91",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "base = \"D:\\\\projects\\\\LLMValueInvestigation\\\\code\\\\human_evaluation\\\\survey_data\\\\AECE_v1\\\\Originals\\\\CN_survey\\\\supplyment data\"\n",
    "\n",
    "css_data = pd.read_csv(f\"{base}\\\\CGSS_female.csv\")\n",
    "wvs_data = pd.read_csv(f\"{base}\\\\WVS_female.csv\")\n",
    "out_data = pd.concat([wvs_data, css_data], axis=1)\n",
    "out_data = pd.concat([out_data, out_data], axis=1)\n",
    "out_data.to_csv(\"female.csv\", index=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ebf9927b-1afc-4076-a23e-5399200ec2ce",
   "metadata": {},
   "source": [
    "# Gather Character data\n",
    "with open(\"D:\\\\projects\\\\LLMValueInvestigation\\\\Chinese_questionaire.txt\", \"r\", encoding='utf-8') as q_f:\n",
    "    questions = q_f.read()\n",
    "    ques_list = questions.split('\\n')\n",
    "\n",
    "with open(\"D:\\\\projects\\\\LLMValueInvestigation\\\\code\\\\human_evaluation\\\\survey_data\\\\AECE\\\\Chinese_questionaires.txt\", \"r\", encoding='utf-8') as o_f:\n",
    "    ori_ques = o_f.read()\n",
    "    ori_list = ori_ques.split(\"\\n\")\n",
    "\n",
    "mode = \"50\"\n",
    "index_list = []\n",
    "for ques in ques_list:\n",
    "    index_list.append(ori_list.index(ques))\n",
    "indexs = [ i+1 for i in index_list]\n",
    "\n",
    "chara = pd.read_csv(\n",
    "    f\"D:\\\\projects\\\\LLMValueInvestigation\\\\code\\\\human_evaluation\\\\survey_data\\\\AECE_v1\\\\Originals\\\\CN_survey\\\\supplyment data\\\\Character\\\\{mode}.csv\", \n",
    "    index_col=0)\n",
    "\n",
    "extra_list = []\n",
    "for ind in indexs:\n",
    "    if ind not in range(210, 215):\n",
    "        extra_list.append(chara[str(ind)].values)\n",
    "    else:\n",
    "        length = len(chara[f\"{str(ind)}-1\"].values)\n",
    "        extra_list.append([100]*length)\n",
    "\n",
    "extr_data = pd.DataFrame(extra_list, index=range(441, 440 + len(extra_list) + 1)).T\n",
    "\n",
    "final_chara = pd.concat([chara, extr_data], axis=1)\n",
    "final_chara.to_excel(f\"final_{mode}.xlsx\", index=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "43250eba-6af7-48a4-a259-058122fff1e7",
   "metadata": {},
   "source": [
    "# Load human preference data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "base = \"D:\\\\projects\\\\LLMValueInvestigation\\\\code\\\\human_evaluation\\\\human_eval\\\\\"\n",
    "test_type = \"US_survey\"\n",
    "data_base = \"D:\\\\projects\\\\LLMValueInvestigation\\\\code\\\\human_evaluation\\\\survey_data\\\\AECE_v1\\\\preferences\\\\usa_\"\n",
    "\n",
    "chara_base = \"D:\\\\projects\\\\LLMValueInvestigation\\\\code\\\\human_evaluation\\\\survey_data\\\\AECE_v1\\\\Originals\"\n",
    "all_data = pd.read_csv(f\"{chara_base}\\\\{test_type}\\\\Characters\\\\final_all.csv\", dtype=np.float32)\n",
    "\n",
    "male_data = pd.read_csv(f\"{chara_base}\\\\{test_type}\\\\Characters\\\\final_male.csv\",  dtype=np.float32)\n",
    "female_data = pd.read_csv(f\"{chara_base}\\\\{test_type}\\\\Characters\\\\final_female.csv\", dtype=np.float32)\n",
    "\n",
    "_29_data = pd.read_csv(f\"{chara_base}\\\\{test_type}\\\\Characters\\\\final_29.csv\", dtype=np.float32)\n",
    "_30_data = pd.read_csv(f\"{chara_base}\\\\{test_type}\\\\Characters\\\\final_30.csv\", dtype=np.float32)\n",
    "_50_data = pd.read_csv(f\"{chara_base}\\\\{test_type}\\\\Characters\\\\final_50.csv\", dtype=np.float32)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "efee316b-9cb8-4983-aba2-c36ec8532f4e",
   "metadata": {},
   "source": [
    "# Prepare Human Character\n",
    "vars = all_data.columns\n",
    "\n",
    "def process_dis(data, v):\n",
    "    _var = data.loc[:, v]\n",
    "    # _var = _var[~_var.isin([\"iap\"])]\n",
    "    _count = _var.value_counts()\n",
    "    return _count.sort_index()\n",
    "\n",
    "def get_count(counts, id):\n",
    "    if id in counts.index:\n",
    "        return counts[id]\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "var_chara = []\n",
    "for var in vars:\n",
    "    all_count = process_dis(all_data, var)\n",
    "    male_count = process_dis(male_data, var)\n",
    "    female_count = process_dis(female_data, var)\n",
    "\n",
    "    _29_count = process_dis(_29_data, var)\n",
    "    _30_count = process_dis(_30_data, var)\n",
    "    _50_count = process_dis(_50_data, var)\n",
    "\n",
    "    choice_list =pd.DataFrame(columns=[\"Sex\", \"Age\"], index=all_count.index)\n",
    "    for ind in all_count.index:\n",
    "        if ind >= 0:\n",
    "            \n",
    "            m_count = get_count(male_count, ind)\n",
    "            f_count = get_count(female_count, ind)\n",
    "            _2_count = get_count(_29_count, ind)\n",
    "            _3_count = get_count(_30_count, ind)\n",
    "            _5_count = get_count(_50_count, ind)\n",
    "            \n",
    "            if m_count > f_count:\n",
    "                choice_list.loc[ind][\"Sex\"] = \"M\"\n",
    "            elif f_count > m_count:\n",
    "                choice_list.loc[ind][\"Sex\"] = \"F\"\n",
    "            else:\n",
    "                choice_list.loc[ind][\"Sex\"] = \"T\"\n",
    "     \n",
    "            if len(set([_2_count, _3_count, _5_count])) == 3:\n",
    "                if max(_2_count, _3_count, _5_count) == _2_count:\n",
    "                    choice_list.loc[ind][\"Age\"] = \"29\"\n",
    "                elif max(_2_count, _3_count, _5_count) == _3_count:\n",
    "                    choice_list.loc[ind][\"Age\"] = \"30\"\n",
    "                elif max(_2_count, _3_count, _5_count) == _5_count:\n",
    "                    choice_list.loc[ind][\"Age\"] = \"50\"\n",
    "            elif _2_count == _3_count and max(_2_count, _3_count, _5_count) == _2_count:\n",
    "                choice_list.loc[ind][\"Age\"] = \"29-30\"\n",
    "            elif _2_count == _5_count and max(_2_count, _3_count, _5_count) == _2_count:\n",
    "                choice_list.loc[ind][\"Age\"] = \"29-50\"\n",
    "            elif _5_count == _3_count and max(_2_count, _3_count, _5_count) == _5_count:\n",
    "                choice_list.loc[ind][\"Age\"] = \"30-50\"\n",
    "            else:\n",
    "                choice_list.loc[ind][\"Age\"] = \"T\"\n",
    "            # print(choice_list)\n",
    "    var_chara.append(choice_list.dropna())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e1e3934a-6154-49f1-89a8-35c8c392947b",
   "metadata": {},
   "source": [
    "var_chara_data = pd.concat(var_chara, keys=[index.index.name for index in var_chara])\n",
    "var_chara_data.to_excel(f\"{chara_base}\\\\{test_type}\\\\Character.xlsx\", index=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2778606c-52d5-4841-830f-92e9d24e5c0a",
   "metadata": {},
   "source": [
    "# PCA and CulMap\n",
    "def prepare_pca_data(theme_ques, final_data):\n",
    "    theme_mean_list = []\n",
    "    for theme in theme_ques:\n",
    "        theme_sum = 0\n",
    "        theme_count = 0\n",
    "        for ques in theme:\n",
    "            if ques in final_data.index:\n",
    "                if ques not in [7,8] and final_data.loc[ques].sum() != 0:\n",
    "                    q_mean = 0\n",
    "                    for choice in final_data.loc[ques].index:\n",
    "                        if choice != \"12\":\n",
    "                            q_mean += float(choice) * (final_data.loc[ques][choice] / final_data.loc[ques].sum())\n",
    "                    theme_sum += q_mean\n",
    "                    theme_count+=1\n",
    "        # print(f\"final_data.loc[ques]: {final_data.loc[ques]}\")\n",
    "        # print(f\"ques: {ques}\")\n",
    "        if theme_count != 0:\n",
    "            theme_mean = theme_sum / theme_count\n",
    "        else:\n",
    "            theme_mean = 0\n",
    "        theme_mean_list.append(theme_mean)\n",
    "    return theme_mean_list\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import numpy\n",
    "\n",
    "model_name = [\"Baichuan2-13B-Chat\",\"WizardLM-13B\",\"ChatGLM2-6B\", \"Mistral-7B-Instruct\",\"Dolphin-2.2.1-Mistral-7B\",\"Mixtral-8x7B-Instruct\", \"Llama-3-8B\",\n",
    "              \"Llama-3-8B-Instruct\",\"Dolphin-2.9.1-Llama-3-8B\", \"Llama-3-Chinese-8B-Instruct\", \"Claude-3.5-Sonnet\"]\n",
    "\n",
    "features = []\n",
    "feature_name = []\n",
    "\n",
    "cn_question_type = pd.read_excel(\n",
    "        \"F:\\\\Projects\\\\LLMValueInvestigation\\\\code\\\\human_evaluation\\\\survey_data\\\\AECE_v1\\\\Originals\\\\CN_survey\\\\result_database\\\\Question_type.xlsx\")\n",
    "type_index = cn_question_type.groupby(\"value type\")\n",
    "cn_theme_index = []\n",
    "for index, value in type_index:\n",
    "    cn_theme_index.append(value[\"id\"].values.tolist())\n",
    "\n",
    "cn_question_tp = cn_question_type\n",
    "type_id = cn_question_tp.groupby(\"value type\")\n",
    "cn_theme_id = []\n",
    "for index, value in type_id:\n",
    "    cn_theme_id.append(value[\"id\"].values.tolist())\n",
    "\n",
    "\n",
    "us_question_type = pd.read_excel(\n",
    "    \"F:\\\\Projects\\\\LLMValueInvestigation\\\\code\\\\human_evaluation\\\\survey_data\\\\AECE_v1\\\\Originals\\\\US_survey\\\\result_database\\\\Question_type.xlsx\")\n",
    "type_index = us_question_type.groupby(\"value type\")\n",
    "us_theme_index = []\n",
    "for index, value in type_index:\n",
    "    us_theme_index.append(value[\"id\"].values.tolist())\n",
    "\n",
    "us_question_tp = us_question_type\n",
    "type_id = us_question_tp.groupby(\"value type\")\n",
    "us_theme_id = []\n",
    "for index, value in type_id:\n",
    "    us_theme_id.append(value[\"id\"].values.tolist())\n",
    "\n",
    "    \n",
    "\n",
    "zh_ref_data = pd.read_csv(\n",
    "        \"F:\\\\Projects\\\\LLMValueInvestigation\\\\code\\\\human_evaluation\\\\survey_data\\\\AECE_v1\\\\preferences\\\\zh_ref_score_all.csv\",\n",
    "        index_col=0).fillna(0)\n",
    "en_ref_data = pd.read_csv(\n",
    "    \"F:\\\\Projects\\\\LLMValueInvestigation\\\\code\\\\human_evaluation\\\\survey_data\\\\AECE_v1\\\\preferences\\\\usa_ref_score_all.csv\", \n",
    "    index_col=0).fillna(0)\n",
    "\n",
    "zh_ref_theme_mean = prepare_pca_data(theme_ques=cn_theme_index, final_data=zh_ref_data)\n",
    "en_ref_theme_mean = prepare_pca_data(theme_ques=us_theme_index, final_data=en_ref_data)\n",
    "features.append(en_ref_theme_mean)\n",
    "features.append(zh_ref_theme_mean)\n",
    "feature_name.append(\"Reference US\")\n",
    "feature_name.append(\"Reference CN\")\n",
    "\n",
    "for name in model_name:\n",
    "    cn_score_base = \"F:\\\\Projects\\\\LLMValueInvestigation\\\\code\\\\human_evaluation\\\\human_eval\\\\CN_survey_all\\\\\"\n",
    "    us_score_base = \"F:\\\\Projects\\\\LLMValueInvestigation\\\\code\\\\human_evaluation\\\\human_eval\\\\US_survey_all\\\\\"\n",
    "    zh_final = pd.read_csv(cn_score_base + name + \"\\\\final_score.csv\", index_col=0)\n",
    "    en_final = pd.read_csv(us_score_base + name + \"\\\\final_score.csv\", index_col=0)\n",
    "    # if \"Llama\" in name:\n",
    "    #     cn_theme_index = cn_theme_id\n",
    "    #     us_theme_index = us_theme_id\n",
    "    print(f\"name: {name}\")\n",
    "    zh_theme_mean = prepare_pca_data(theme_ques=cn_theme_index, final_data=zh_final)\n",
    "    en_theme_mean = prepare_pca_data(theme_ques=us_theme_index, final_data=en_final)\n",
    "    features.append(en_theme_mean)\n",
    "    feature_name.append(f\"{name} US\")  \n",
    "    features.append(zh_theme_mean)\n",
    "    feature_name.append(f\"{name} CN\")  \n",
    "\n",
    "A = numpy.array(features)\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(A)\n",
    "result = pca.transform(A)\n",
    "\n",
    "pca_datapoint = {\n",
    "    \"Survival v.s. Self-Expression\": result[:,0],\n",
    "    \"Traditional v.s. Secular\": result[:,1],\n",
    "    \"Model type\": feature_name\n",
    "}\n",
    "cord = pd.DataFrame(pca_datapoint, index=feature_name)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c840f845-a04e-42be-9bd2-80bfbeea9850",
   "metadata": {},
   "source": [
    "c1 = sns.color_palette(\"husl\", as_cmap=True,).resampled(12).colors # CN\n",
    "c2 = sns.color_palette(\"hls\", as_cmap=True,).resampled(12).colors # US\n",
    "color_list = []\n",
    "for cc1, cc2 in zip(c1, c2):\n",
    "    color_list.append(cc2)\n",
    "    color_list.append(cc1)\n",
    "mixed_pal = sns.color_palette(color_list)\n",
    "mixed_pal"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "13929478-12a9-4a3f-8675-aca3106b59be",
   "metadata": {},
   "source": [
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "sns.set_context(\"paper\")\n",
    "p1 = sns.scatterplot(\n",
    "    x=cord[\"Survival v.s. Self-Expression\"], \n",
    "    y=cord[\"Traditional v.s. Secular\"], \n",
    "    data=cord, hue=cord[\"Model type\"], \n",
    "    palette=mixed_pal,sizes=[100]\n",
    ")\n",
    "\n",
    "for line in range(0,cord.shape[0]):\n",
    "    if \"CN\" in cord.index[line] or \"Reference\" in cord.index[line]:\n",
    "         p1.text(\n",
    "             cord[\"Survival v.s. Self-Expression\"][line],\n",
    "             cord[\"Traditional v.s. Secular\"][line] + random.uniform(-0.1, 0.1),\n",
    "             cord.index[line], horizontalalignment='center', size='x-small', color='black'\n",
    "         )\n",
    "        \n",
    "model_name = [\"Reference\", \"Baichuan2-13B-Chat\",\"WizardLM-13B\",\"ChatGLM2-6B\", \"Mistral-7B-Instruct\",\"Dolphin-2.2.1-Mistral-7B\",\"Mixtral-8x7B-Instruct\", \"Llama-3-8B\",\n",
    "              \"Llama-3-8B-Instruct\",\"Dolphin-2.9.1-Llama-3-8B\", \"Llama-3-Chinese-8B-Instruct\", \"Claude-3.5-Sonnet\"]\n",
    "\n",
    "colors= [\"#e96870\", \"#df9545\", \"#bfbb44\", \"#87c144\",\"#45c667\", \"#47c5a9\", \"#46aec6\", \"#4888d8\", \"#8e75e8\", \"#d763e8\", \"#e85eb1\", \"#e86671\"]\n",
    "custom = sns.color_palette(colors)\n",
    "\n",
    "line_dict = {\n",
    "    \"x\": [],\n",
    "    \"y\": [],\n",
    "    \"Model type\": [],\n",
    "}\n",
    "for name in model_name:\n",
    "    cn_cord = cord[cord[\"Model type\"] == f\"{name} CN\"]\n",
    "    from_x = cn_cord[\"Survival v.s. Self-Expression\"].item()\n",
    "    from_y = cn_cord[\"Traditional v.s. Secular\"].item()\n",
    "    line_dict[\"x\"].append(from_x)\n",
    "    line_dict[\"y\"].append(from_y)\n",
    "    line_dict[\"Model type\"].append(name)\n",
    "    us_cord = cord[cord[\"Model type\"] == f\"{name} US\"]\n",
    "    to_x = us_cord[\"Survival v.s. Self-Expression\"].item()\n",
    "    to_y = us_cord[\"Traditional v.s. Secular\"].item()\n",
    "    line_dict[\"x\"].append(to_x)\n",
    "    line_dict[\"y\"].append(to_y)\n",
    "    line_dict[\"Model type\"].append(name)\n",
    "    \n",
    "line_data = pd.DataFrame(line_dict)\n",
    "sns.lineplot(line_data, x=\"x\", y=\"y\", hue=\"Model type\", palette=custom, linestyle='-.')\n",
    "sns.move_legend(p1, \"upper left\", bbox_to_anchor=(1, 1), ncols=2)\n",
    "dot_fig = p1.get_figure()\n",
    "dot_fig.savefig(\"culmap_labels.pdf\", bbox_inches='tight')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "feabc0ff-b045-4cbb-93db-d2176af0ff42",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Profile Drawing for Model\n",
    "data_base = \"F:\\\\Projects\\\\LLMValueInvestigation\\\\code\\\\human_evaluation\\\\survey_data\\\\AECE_v1\\\\preferences\\\\\"\n",
    "name = \"Mixtral-8x7B-Instruct-v0.1-bnb-4bit\"\n",
    "test_type = \"CN_survey\"\n",
    "lang_short = \"zh\"\n",
    "\n",
    "var_chara_data = pd.read_excel(f\"F:\\\\Projects\\\\LLMValueInvestigation\\\\code\\\\human_evaluation\\\\survey_data\\\\AECE_v1\\\\Originals\\\\{test_type}\\\\Character.xlsx\", index_col=[0, 1])\n",
    "base = f\"F:\\\\Projects\\\\LLMValueInvestigation\\\\code\\\\human_evaluation\\\\human_eval\\\\{test_type}_all\"\n",
    "human_res_data = pd.read_csv(f\"{data_base}{lang_short}_ref_score_all.csv\")[1:].drop('12', axis=1)\n",
    "model_res_data = pd.read_csv(f\"{base}\\\\{name}\\\\final_score.csv\", index_col=0)[1:].drop('12', axis=1)\n",
    "\n",
    "profile_data = pd.DataFrame(columns=[\"Match\", \"Sex\", \"Age\"], index=range(1,614))\n",
    "multiple_choice_ques = [7, 8, 227, 228]\n",
    "\n",
    "for i in range(1, len(model_res_data) + 1):\n",
    "    # print(i)\n",
    "    model_prefer = model_res_data.loc[i].idxmax()\n",
    "    human_prefer = human_res_data.loc[i].idxmax()\n",
    "    if model_prefer == human_prefer:\n",
    "        profile_data.loc[i][\"Match\"] = \"Y\"\n",
    "    else:\n",
    "        profile_data.loc[i][\"Match\"] = \"N\"\n",
    "    if i not in multiple_choice_ques:\n",
    "        # print(var_chara_data.index)\n",
    "        if (str(i), float(model_prefer)) in var_chara_data.index:\n",
    "            profile_data.loc[i][\"Sex\"] = var_chara_data.loc[str(i), float(model_prefer)][\"Sex\"]\n",
    "            profile_data.loc[i][\"Age\"] = var_chara_data.loc[str(i), int(model_prefer)][\"Age\"]\n",
    "        else:\n",
    "            profile_data.loc[i][\"Sex\"] = \"Null\"\n",
    "            profile_data.loc[i][\"Age\"] = \"Null\"\n",
    "    else:\n",
    "        if (f\"{str(i)}-{int(model_prefer)}\", 1.0) in var_chara_data.index:\n",
    "            profile_data.loc[i][\"Sex\"] = var_chara_data.loc[f\"{str(i)}-{int(model_prefer)}\", 1.0][\"Sex\"]\n",
    "            profile_data.loc[i][\"Age\"] = var_chara_data.loc[f\"{str(i)}-{int(model_prefer)}\", 1.0][\"Age\"]\n",
    "        else:\n",
    "            profile_data.loc[i][\"Sex\"] = \"Null\"\n",
    "            profile_data.loc[i][\"Age\"] = \"Null\"\n",
    "        # print(profile_data.loc[i])\n",
    "chara_base = \"F:\\\\Projects\\\\LLMValueInvestigation\\\\code\\\\human_evaluation\\\\survey_data\\\\AECE_v1\\\\Originals\"\n",
    "question_type = pd.read_excel(\n",
    "    f\"{chara_base}\\\\{test_type}\\\\result_database\\\\Question_type.xlsx\"\n",
    ")\n",
    "types = question_type[\"value type\"]\n",
    "types.index = range(1, len(types) + 1)\n",
    "profile_data = pd.concat([profile_data, types], axis=1,)\n",
    "# profile_data\n",
    "profile_data.to_excel(f\"{base}\\\\{name}\\\\Character.xlsx\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "aa32aac7-c1e5-45e3-8ac0-d42aaa625ebf",
   "metadata": {},
   "source": [
    "# Matching Profile barplot\n",
    "import seaborn as sns\n",
    "base = \"F:\\\\Projects\\\\LLMValueInvestigation\\\\code\\\\human_evaluation\\\\human_eval\\\\\"\n",
    "test_type = \"US_survey_all\"\n",
    "\n",
    "model_name = [\"Baichuan2-13B-Chat\",\"WizardLM-13B\",\"ChatGLM2-6B\", \"Mistral-7B-Instruct\",\"Dolphin-2.2.1-Mistral-7B\",\"Mixtral-8x7B-Instruct\", \"Llama-3-8B\",\n",
    "              \"Llama-3-8B-Instruct\",\"Dolphin-2.9.1-Llama-3-8B\", \"Llama-3-Chinese-8B-Instruct\", \"Claude-3.5-Sonnet\"]\n",
    "\n",
    "v_match_list = []\n",
    "model_list = []\n",
    "v_list = []\n",
    "vs = [\"EVI\", \"IDV\", \"IVR\", \"LTO\",\"MAS\",\"PDI\",\"TDI\",\"UAI\",\"USI\"]\n",
    "for name in model_name:\n",
    "    m_chara = pd.read_excel(f\"{base}{test_type}\\\\{name}\\\\Character.xlsx\", index_col=0)\n",
    "    m_chara_v = m_chara.groupby(\"value type\")\n",
    "    for v, m_c_v in m_chara_v:\n",
    "        model_list.append(name)\n",
    "        v_list.append(v)\n",
    "        if \"Y\" in m_c_v[\"Match\"].value_counts().index:\n",
    "            v_match_list.append(m_c_v[\"Match\"].value_counts()[\"Y\"]/len(m_c_v[\"Match\"])*100)\n",
    "        else:\n",
    "            v_match_list.append(0)\n",
    "\n",
    "cn_overall_match = {\n",
    "    \"Model\": model_list,\n",
    "    \"Precentage of match\": v_match_list,\n",
    "    # \"Mismatch\": [540]*len(model_list),\n",
    "    \"Value type\": v_list\n",
    "}\n",
    "cn_overall_match_data = pd.DataFrame(cn_overall_match)\n",
    "s2 = sns.barplot(data=cn_overall_match_data, x=\"Value type\", y=\"Precentage of match\", hue=\"Model\", palette=\"tab20c\")\n",
    "sns.move_legend(s2, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "barfig = s2.get_figure()\n",
    "barfig.savefig(f\"{test_type}_matching.pdf\", bbox_inches='tight')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e63fa4e6",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as matlib\n",
    "\n",
    "sns.set()\n",
    "sns.set_context('paper')\n",
    "# sns.set_style(\"whitegrid\")\n",
    "of_base = \"F:\\\\Projects\\\\LLMValueInvestigation\\\\code\\\\human_evaluation\\\\human_eval\\\\overall_profile.csv\"\n",
    "overall_profiles = pd.read_csv(of_base)\n",
    "model_name = [\"Mixtral-8x7B-Instruct\"]\n",
    "\n",
    "m_o_p = overall_profiles.groupby(\"Model\")\n",
    "\n",
    "for m, overall_profile in m_o_p:\n",
    "    \n",
    "    mat_overall = overall_profile[overall_profile[\"Profile\"]==\"matching\"]\n",
    "    mat_sex = mat_overall[mat_overall[\"Character type\"]==\"sex\"]\n",
    "    \n",
    "    mat_us_total = mat_sex[mat_sex[\"Test type\"]==\"US survey\"][\"Number\"].sum()\n",
    "    mat_us_male = mat_sex[mat_sex[\"Test type\"]==\"US survey\"][mat_sex[\"Character\"]==\"Male\"][\"Number\"].tolist()[0]\n",
    "    mat_us_female = mat_sex[mat_sex[\"Test type\"]==\"US survey\"][mat_sex[\"Character\"]==\"Female\"][\"Number\"].tolist()[0]\n",
    "    \n",
    "    mat_cn_total = mat_sex[mat_sex[\"Test type\"]==\"CN survey\"][\"Number\"].sum()\n",
    "    mat_cn_male = mat_sex[mat_sex[\"Test type\"]==\"CN survey\"][mat_sex[\"Character\"]==\"Male\"][\"Number\"].tolist()[0]\n",
    "    mat_cn_female = mat_sex[mat_sex[\"Test type\"]==\"CN survey\"][mat_sex[\"Character\"]==\"Female\"][\"Number\"].tolist()[0]\n",
    "    \n",
    "    mis_overall = overall_profile[overall_profile[\"Profile\"]==\"mismatch\"]\n",
    "    mis_sex = mis_overall[mis_overall[\"Character type\"]==\"sex\"]\n",
    "    \n",
    "    mis_us_total = mis_sex[mis_sex[\"Test type\"]==\"US survey\"][\"Number\"].sum()\n",
    "    mis_us_male = mis_sex[mis_sex[\"Test type\"]==\"US survey\"][mis_sex[\"Character\"]==\"Male\"][\"Number\"].tolist()[0]\n",
    "    mis_us_female = mis_sex[mis_sex[\"Test type\"]==\"US survey\"][mis_sex[\"Character\"]==\"Female\"][\"Number\"].tolist()[0]\n",
    "    \n",
    "    mis_cn_total = mis_sex[mis_sex[\"Test type\"]==\"CN survey\"][\"Number\"].sum()\n",
    "    mis_cn_male = mis_sex[mis_sex[\"Test type\"]==\"CN survey\"][mis_sex[\"Character\"]==\"Male\"][\"Number\"].tolist()[0]\n",
    "    mis_cn_female = mis_sex[mis_sex[\"Test type\"]==\"CN survey\"][mis_sex[\"Character\"]==\"Female\"][\"Number\"].tolist()[0]\n",
    "    \n",
    "    plt.figure(figsize=(2,1))\n",
    "    fig,axes=plt.subplots(1,2)\n",
    "    \n",
    "    mat_total_data = pd.DataFrame({\n",
    "        \"Test type\": [\"US survey\", \"CN survey\"],\n",
    "        \"Percentage\": [mat_us_total/mat_us_total*100, mat_cn_total/mat_cn_total*100]\n",
    "    })\n",
    "    \n",
    "    mat_top_plt = sns.barplot(data=mat_total_data, x=\"Test type\", y=\"Percentage\", color=\"#9BB5F0\", ax=axes[0])\n",
    "    \n",
    "    mat_bottom_data = pd.DataFrame({\n",
    "        \"Test type\": [\"US survey\", \"CN survey\"],\n",
    "        \"Percentage\": [mat_us_female/mat_us_total*100, mat_cn_female/mat_cn_total*100]\n",
    "    })\n",
    "    \n",
    "    mat_bottom_plt = sns.barplot(data=mat_bottom_data, x=\"Test type\", y=\"Percentage\", color=\"#E5A089\", ax=axes[0])\n",
    "    \n",
    "    mis_total_data = pd.DataFrame({\n",
    "        \"Test type\": [\"US survey\", \"CN survey\"],\n",
    "        \"Percentage\": [mis_us_total/mis_us_total*100, mis_cn_total/mis_cn_total*100]\n",
    "    })\n",
    "    \n",
    "    mis_top_plt = sns.barplot(data=mis_total_data, x=\"Test type\", y=\"Percentage\", color=\"#9BB5F0\",  ax=axes[1])\n",
    "    \n",
    "    mis_bottom_data = pd.DataFrame({\n",
    "        \"Test type\": [\"US survey\", \"CN survey\"],\n",
    "        \"Percentage\": [mis_us_female/mis_us_total*100, mis_cn_female/mis_cn_total*100]\n",
    "    })\n",
    "    \n",
    "    mis_bottom_plt = sns.barplot(data=mis_bottom_data, x=\"Test type\", y=\"Percentage\", color=\"#E5A089\",  ax=axes[1])\n",
    "    \n",
    "    axes[0].set_xlabel(\"Match\")\n",
    "    axes[1].set_xlabel(\"Mismatch\")\n",
    "    \n",
    "    axes[0].set_facecolor(\"1\")\n",
    "    axes[1].set_facecolor(\".95\")\n",
    "    \n",
    "    topbar = matlib.patches.Rectangle((0,0),1,1,fc=\"#9BB5F0\", edgecolor = 'none')\n",
    "    bottombar = matlib.patches.Rectangle((0,0),1,1,fc='#E5A089',  edgecolor = 'none')\n",
    "    mis_bottom_plt.legend(handles=[topbar, bottombar], labels=['Male', 'Female'], \n",
    "                          loc='center left', bbox_to_anchor=(1, 0.5), frameon=False, facecolor=\"white\", title=\"Sex\")\n",
    "    plt.savefig(f\"{m}_overall_profile_sex.pdf\", bbox_inches='tight')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c5809fc5",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as matlib\n",
    "\n",
    "sns.set()\n",
    "sns.set_context('paper')\n",
    "# sns.set_style(\"whitegrid\")\n",
    "of_base = \"F:\\\\Projects\\\\LLMValueInvestigation\\\\code\\\\human_evaluation\\\\human_eval\\\\overall_profile.csv\"\n",
    "overall_profiles = pd.read_csv(of_base)\n",
    "model_name = [\"Claude-3.5-Sonnet\"]\n",
    "\n",
    "m_o_p = overall_profiles.groupby(\"Model\")\n",
    "\n",
    "for m, overall_profile in m_o_p:\n",
    "    mat_overall = overall_profile[overall_profile[\"Profile\"]==\"matching\"]\n",
    "    mat_age = mat_overall[mat_overall[\"Character type\"]==\"age\"]\n",
    "    \n",
    "    mat_us_total = mat_age[mat_age[\"Test type\"]==\"US survey\"][\"Number\"].sum()\n",
    "    mat_us_29 = mat_age[mat_age[\"Test type\"]==\"US survey\"][mat_age[\"Character\"]==\"Up to 29\"][\"Number\"].tolist()[0]\n",
    "    mat_us_30 = mat_age[mat_age[\"Test type\"]==\"US survey\"][mat_age[\"Character\"]==\"30 to 49\"][\"Number\"].tolist()[0]\n",
    "    mat_us_50 = mat_age[mat_age[\"Test type\"]==\"US survey\"][mat_age[\"Character\"]==\"Over 50\"][\"Number\"].tolist()[0]\n",
    "    \n",
    "    mat_cn_total = mat_age[mat_age[\"Test type\"]==\"CN survey\"][\"Number\"].sum()\n",
    "    mat_cn_29 = mat_age[mat_age[\"Test type\"]==\"CN survey\"][mat_age[\"Character\"]==\"Up to 29\"][\"Number\"].tolist()[0]\n",
    "    mat_cn_30 = mat_age[mat_age[\"Test type\"]==\"CN survey\"][mat_age[\"Character\"]==\"30 to 49\"][\"Number\"].tolist()[0]\n",
    "    mat_cn_50 = mat_age[mat_age[\"Test type\"]==\"CN survey\"][mat_age[\"Character\"]==\"Over 50\"][\"Number\"].tolist()[0]\n",
    "    \n",
    "    mis_overall = overall_profile[overall_profile[\"Profile\"]==\"mismatch\"]\n",
    "    mis_age = mis_overall[mis_overall[\"Character type\"]==\"age\"]\n",
    "    \n",
    "    mis_us_total = mis_age[mis_age[\"Test type\"]==\"US survey\"][\"Number\"].sum()\n",
    "    mis_us_29 = mis_age[mis_age[\"Test type\"]==\"US survey\"][mis_age[\"Character\"]==\"Up to 29\"][\"Number\"].tolist()[0]\n",
    "    mis_us_30 = mis_age[mis_age[\"Test type\"]==\"US survey\"][mis_age[\"Character\"]==\"30 to 49\"][\"Number\"].tolist()[0]\n",
    "    mis_us_50 = mis_age[mis_age[\"Test type\"]==\"US survey\"][mis_age[\"Character\"]==\"Over 50\"][\"Number\"].tolist()[0]\n",
    "    \n",
    "    mis_cn_total = mis_age[mis_age[\"Test type\"]==\"CN survey\"][\"Number\"].sum()\n",
    "    mis_cn_29 = mis_age[mis_age[\"Test type\"]==\"CN survey\"][mis_age[\"Character\"]==\"Up to 29\"][\"Number\"].tolist()[0]\n",
    "    mis_cn_30 = mis_age[mis_age[\"Test type\"]==\"CN survey\"][mis_age[\"Character\"]==\"30 to 49\"][\"Number\"].tolist()[0]\n",
    "    mis_cn_50 = mis_age[mis_age[\"Test type\"]==\"CN survey\"][mis_age[\"Character\"]==\"Over 50\"][\"Number\"].tolist()[0]\n",
    "    \n",
    "    plt.figure(figsize=(2.5,1))\n",
    "    fig,axes=plt.subplots(1,2)\n",
    "    \n",
    "    mat_total_data = pd.DataFrame({\n",
    "        \"Test type\": [\"US survey\", \"CN survey\"],\n",
    "        \"Percentage\": [mat_us_total/mat_us_total*100, mat_cn_total/mat_cn_total*100]\n",
    "    })\n",
    "    mat_top_plt = sns.barplot(data=mat_total_data, x=\"Test type\", y=\"Percentage\", color=\"#9BB5F0\", ax=axes[0])\n",
    "    \n",
    "    mat_middle_data = pd.DataFrame({\n",
    "        \"Test type\": [\"US survey\", \"CN survey\"],\n",
    "        \"Percentage\": [(mat_us_30+mat_us_50)/mat_us_total*100, (mat_cn_30+mat_cn_50)/mat_cn_total*100]\n",
    "    })\n",
    "    mat_middle_plt = sns.barplot(data=mat_middle_data, x=\"Test type\", y=\"Percentage\", color=\"#DDDCDC\", ax=axes[0])\n",
    "    \n",
    "    mat_bottom_data = pd.DataFrame({\n",
    "        \"Test type\": [\"US survey\", \"CN survey\"],\n",
    "        \"Percentage\": [mat_us_50/mat_us_total*100, mat_cn_50/mat_cn_total*100]\n",
    "    })\n",
    "    mat_bottom_plt = sns.barplot(data=mat_bottom_data, x=\"Test type\", y=\"Percentage\", color=\"#E5A089\", ax=axes[0])\n",
    "    \n",
    "    \n",
    "    mis_total_data = pd.DataFrame({\n",
    "        \"Test type\": [\"US survey\", \"CN survey\"],\n",
    "        \"Percentage\": [mis_us_total/mis_us_total*100, mis_cn_total/mis_cn_total*100]\n",
    "    })\n",
    "    \n",
    "    mis_top_plt = sns.barplot(data=mis_total_data, x=\"Test type\", y=\"Percentage\", color=\"#9BB5F0\",  ax=axes[1])\n",
    "    \n",
    "    mis_middle_data = pd.DataFrame({\n",
    "        \"Test type\": [\"US survey\", \"CN survey\"],\n",
    "        \"Percentage\": [(mis_us_30+mis_us_50)/mis_us_total*100, (mis_cn_30+mis_cn_50)/mis_cn_total*100]\n",
    "    })\n",
    "    mis_middle_plt = sns.barplot(data=mis_middle_data, x=\"Test type\", y=\"Percentage\", color=\"#DDDCDC\", ax=axes[1])\n",
    "    \n",
    "    mis_bottom_data = pd.DataFrame({\n",
    "        \"Test type\": [\"US survey\", \"CN survey\"],\n",
    "        \"Percentage\": [mis_us_50/mis_us_total*100, mis_cn_50/mis_cn_total*100]\n",
    "    })\n",
    "    mis_bottom_plt = sns.barplot(data=mis_bottom_data, x=\"Test type\", y=\"Percentage\", color=\"#E5A089\", ax=axes[1])\n",
    "    axes[0].set_xlabel(\"Match\")\n",
    "    axes[1].set_xlabel(\"Mismatch\")\n",
    "    axes[0].set_facecolor(\"1\")\n",
    "    axes[1].set_facecolor(\".95\")\n",
    "    \n",
    "    topbar = matlib.patches.Rectangle((0,0),1,1,fc=\"#9BB5F0\", edgecolor = 'none')\n",
    "    middlebar = matlib.patches.Rectangle((0,0),1,1,fc='#DDDCDC', edgecolor = 'none')\n",
    "    bottombar = matlib.patches.Rectangle((0,0),1,1,fc='#E5A089',  edgecolor = 'none')\n",
    "    mis_bottom_plt.legend(handles=[topbar, middlebar, bottombar], labels=['Up to 29', \"30 to 49\", 'Over 50'], \n",
    "                          loc='center left', bbox_to_anchor=(1, 0.5), frameon=False, facecolor=\"white\", title=\"Age\")\n",
    "    \n",
    "    plt.savefig(f\"{m}_overall_profile_age.pdf\", bbox_inches='tight')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b2b72e91-bc47-4c38-b937-1489b9ec225a",
   "metadata": {},
   "source": [
    "import seaborn as sns\n",
    "base = \"F:\\\\Projects\\\\LLMValueInvestigation\\\\code\\\\human_evaluation\\\\human_eval\\\\\"\n",
    "test_type = \"CN_survey_all\"\n",
    "\n",
    "model_name = [\"Mistral-7B-Instruct\",\"Mixtral-8x7B-Instruct\", \"Llama-3-8B\",\n",
    "              \"Dolphin-2.9.1-Llama-3-8B\", \"Claude-3.5-Sonnet\"]\n",
    "\n",
    "v_match_list = []\n",
    "model_list = []\n",
    "v_list = []\n",
    "\n",
    "def count(_count_, _count, _count_type, tie_, keys, va):\n",
    "    if ('N', _count_type, keys, va) in _count.index:\n",
    "        _count_ += _count[('N', _count_type, keys, va)]\n",
    "    if ('N', 'T', keys, va) in tie_.index:\n",
    "        _count_ += tie_[('N', 'T', keys, va)]\n",
    "    return _count_\n",
    "\n",
    "def gender_count(_count, type, tie, vb):\n",
    "    _count_29 = 0\n",
    "    _count_30 = 0\n",
    "    _count_50 = 0\n",
    "    for _29_key in _29_keys:\n",
    "        _count_29 = count(_count_29, _count, type, tie, _29_key, vb)\n",
    "    for _30_key in _30_keys:\n",
    "        _count_30 = count(_count_30, _count, type, tie, _30_key, vb)\n",
    "    for _50_key in _50_keys:\n",
    "        _count_50 = count(_count_50, _count, type, tie, _50_key, vb)\n",
    "    return _count_29, _count_30, _count_50\n",
    "\n",
    "_29_keys = [\"29\", \"29-30\", \"29-50\", \"T\"]\n",
    "_30_keys = [\"30\", \"30-50\", \"29-30\", \"T\"]\n",
    "_50_keys = [\"50\", \"30-50\", \"29-50\", \"T\"]\n",
    "\n",
    "value_list = []\n",
    "sex_list = []\n",
    "age_list = []\n",
    "number_list = []\n",
    "model_list = []\n",
    "\n",
    "for name in model_name:\n",
    "    m_chara = pd.read_excel(f\"{base}{test_type}\\\\{name}\\\\Character.xlsx\", index_col=0)\n",
    "    m_chara = m_chara[m_chara[\"Match\"]==\"N\"]\n",
    "    m_chara_v = m_chara.groupby(\"value type\")\n",
    "    for v, m_c_v in m_chara_v:\n",
    "        m_c_v_male = m_c_v[m_c_v[\"Sex\"] == \"M\"]\n",
    "        m_c_v_tie = m_c_v[m_c_v[\"Sex\"] == \"T\"]\n",
    "        m_c_v_female = m_c_v[m_c_v[\"Sex\"] == \"F\"]\n",
    "        \n",
    "        male_count = m_c_v_male.value_counts()\n",
    "        tie_count = m_c_v_tie.value_counts()\n",
    "        female_count = m_c_v_female.value_counts()\n",
    "        \n",
    "        male_count_29, male_count_30, male_count_50 = gender_count(male_count, \"M\", tie_count, v)\n",
    "        female_count_29, female_count_30, female_count_50 = gender_count(female_count, \"F\", tie_count, v)\n",
    "        \n",
    "        def store_line(va, sa, aa, na, ma):\n",
    "            value_list.append(va)\n",
    "            sex_list.append(sa)\n",
    "            age_list.append(aa)\n",
    "            number_list.append(na)\n",
    "            model_list.append(ma)\n",
    "        store_line(v, \"Male\", \"29\", male_count_29, name)\n",
    "        store_line(v, \"Male\", \"30\", male_count_30, name)\n",
    "        store_line(v, \"Male\", \"50\", male_count_50, name)\n",
    "        \n",
    "        store_line(v, \"Female\", \"29\", female_count_29, name)\n",
    "        store_line(v, \"Female\", \"30\", female_count_30, name)\n",
    "        store_line(v, \"Female\", \"50\", female_count_50, name)\n",
    "\n",
    "mis_prof_data = pd.DataFrame({\n",
    "    \"Value type\": value_list,\n",
    "    \"Sex\": sex_list,\n",
    "    \"Age\": age_list,\n",
    "    \"Number\": number_list,\n",
    "    \"Model\": model_list\n",
    "})\n",
    "\n",
    "mis_prof_data.to_excel(f\"{base}{test_type}\\\\mis_prof_data.xlsx\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d98409e4-a08b-46f1-92c5-faa4724f44c8",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Mismatch profile character\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "base = \"F:\\\\Projects\\\\LLMValueInvestigation\\\\code\\\\human_evaluation\\\\human_eval\\\\\"\n",
    "test_type = \"CN_survey_all\"\n",
    "\n",
    "overall_profile = pd.read_excel(f\"{base}{test_type}\\\\mis_prof_data.xlsx\", index_col=0)\n",
    "\n",
    "topicss = [[\"EVI\", \"IDV\", \"IVR\"], [\"LTO\",\"MAS\",\"PDI\"], [\"TDI\",\"UAI\",\"USI\"]]\n",
    "\n",
    "model_name = [\"Mistral-7B-Instruct\",\"Mixtral-8x7B-Instruct\", \"Llama-3-8B\",\n",
    "              \"Dolphin-2.9.1-Llama-3-8B\", \"Claude-3.5-Sonnet\"]\n",
    "\n",
    "for name in model_name:\n",
    "    fig, ax =plt.subplots(3,3 ,figsize=(90,45))\n",
    "    sns.set()\n",
    "    sns.set_context('paper', font_scale=8)\n",
    "    for j in range(len(topicss)):\n",
    "        topics = topicss[j]\n",
    "        for i in range(len(topics)):\n",
    "            \n",
    "            sample = overall_profile[overall_profile[\"Value type\"]==topics[i]]\n",
    "            sample = sample[sample[\"Model\"]==name]\n",
    "            \n",
    "            heat_data = sample.pivot(index=\"Sex\", columns=\"Age\", values=\"Number\")\n",
    "            axesSub = sns.heatmap(heat_data, annot=True, cmap=sns.cubehelix_palette(as_cmap=True), ax=ax[j][i])\n",
    "            # US color: start=.5, rot=-.5, as_cmap=True, CN color: as_cmap=True\n",
    "            axesSub.set_title(f\"Value type = {topics[i]}\")\n",
    "            axesSub.set_xlabel(\"\")\n",
    "            axesSub.set_ylabel(\"\")\n",
    "    plt.savefig(f\"{test_type}_{name}_topical_profile_heat.pdf\", bbox_inches='tight')\n",
    "    "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "85e19e43-e55f-4498-8b7b-fdba1814c01c",
   "metadata": {},
   "source": [
    "# Value type KL-D lang abalation\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# model_name = [\"Chinese\", \"en_Chinese\"]\n",
    "model_name = [\"English\", \"zh_English\"]\n",
    "\n",
    "mean_data = pd.DataFrame()\n",
    "overall_mean = []\n",
    "for i in range(len(model_name)):\n",
    "    name = model_name[i]\n",
    "    cn_base = \"D:\\\\projects\\\\LLMValueInvestigation\\\\code\\\\human_evaluation\\\\human_eval\\\\lang_abla\\\\\" + name + \"\\\\\"\n",
    "    cn_kld_data = pd.read_csv(cn_base+\"qbq_kld.csv\", index_col=0)\n",
    "    cn_test_type = pd.DataFrame([\"US Survey\"]*len(cn_kld_data), index=range(len(cn_kld_data)), columns=[\"Test type\"])\n",
    "    cn_kld = pd.concat([cn_kld_data, cn_test_type], axis=1)\n",
    "    cn_kld.columns = [\"Num question\", \"KL-Div\", \"Value type\", \"Test type\"]\n",
    "    \n",
    "    cn_means = cn_kld.groupby('Value type')['KL-Div'].mean()\n",
    "    mean = cn_means.mean()\n",
    "    overall_mean.append(mean)\n",
    "    temp = {\n",
    "        \"Value type\": cn_means.index,\n",
    "        \"Mean KL-D\": cn_means.values,\n",
    "        \"Model\": [name] * len(cn_means)\n",
    "    }\n",
    "    \n",
    "    mean_temp = pd.DataFrame(temp)\n",
    "    mean_data = pd.concat([mean_data, mean_temp])\n",
    "# sns.set(\"white\")\n",
    "sns.set_style(\"white\")\n",
    "sns.set_context('paper')\n",
    "# mean_data\n",
    "ax = sns.barplot(data=mean_data, x=\"Value type\", y=\"Mean KL-D\", hue=\"Model\", palette=\"coolwarm\")\n",
    "colors = [\"#8BA5E9\", \"#EACDBE\"]\n",
    "for i in range(len(model_name)): \n",
    "    plt.axhline(y=overall_mean[i],ls=\"-.\",c=colors[i])#\n",
    "sns.move_legend(ax, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "barplt = ax.get_figure()\n",
    "barplt.savefig(f\"us_val_mean_aba.pdf\",  bbox_inches='tight')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ee46f420-877a-4f55-be4c-8285555e6b90",
   "metadata": {},
   "source": [
    "# PCA and CulMap\n",
    "def prepare_pca_data(theme_ques, final_data):\n",
    "    theme_mean_list = []\n",
    "    for theme in theme_ques:\n",
    "        theme_sum = 0\n",
    "        theme_count = 0\n",
    "        for ques in theme:\n",
    "            if ques not in [7,8] and final_data.loc[ques].sum() != 0:\n",
    "                q_mean = 0\n",
    "                for choice in final_data.loc[ques].index:\n",
    "                    if choice != \"12\":\n",
    "                        q_mean += float(choice) * (final_data.loc[ques][choice] / final_data.loc[ques].sum())\n",
    "                theme_sum += q_mean\n",
    "                theme_count+=1\n",
    "        theme_mean = theme_sum / theme_count\n",
    "        theme_mean_list.append(theme_mean)\n",
    "    return theme_mean_list\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd\n",
    "import numpy\n",
    "\n",
    "model_name = [\"Chinese\", \"en_Chinese\", \"English\", \"zh_English\"]\n",
    "features = []\n",
    "feature_name = []\n",
    "\n",
    "cn_question_type = pd.read_excel(\n",
    "        \"D:\\\\projects\\\\LLMValueInvestigation\\\\code\\\\human_evaluation\\\\survey_data\\\\AECE_v1\\\\Originals\\\\CN_survey\\\\result_database\\\\Question_type.xlsx\")[:219]\n",
    "type_index = cn_question_type.groupby(\"value type\")\n",
    "cn_theme_index = []\n",
    "for index, value in type_index:\n",
    "    cn_theme_index.append(value[\"id\"].values.tolist())\n",
    "\n",
    "us_question_type = pd.read_excel(\n",
    "    \"D:\\\\projects\\\\LLMValueInvestigation\\\\code\\\\human_evaluation\\\\survey_data\\\\AECE_v1\\\\Originals\\\\US_survey\\\\result_database\\\\Question_type.xlsx\")[:296]\n",
    "type_index = us_question_type.groupby(\"value type\")\n",
    "us_theme_index = []\n",
    "for index, value in type_index:\n",
    "    us_theme_index.append(value[\"id\"].values.tolist())\n",
    "\n",
    "zh_ref_data = pd.read_csv(\n",
    "        \"D:\\\\projects\\\\LLMValueInvestigation\\\\code\\\\human_evaluation\\\\survey_data\\\\AECE_v1\\\\preferences\\\\zh_ref_score_all.csv\",\n",
    "        index_col=0).fillna(0)\n",
    "en_ref_data = pd.read_csv(\n",
    "    \"D:\\\\projects\\\\LLMValueInvestigation\\\\code\\\\human_evaluation\\\\survey_data\\\\AECE_v1\\\\preferences\\\\usa_ref_score_all.csv\", \n",
    "    index_col=0).fillna(0)\n",
    "\n",
    "zh_ref_theme_mean = prepare_pca_data(theme_ques=cn_theme_index, final_data=zh_ref_data)\n",
    "en_ref_theme_mean = prepare_pca_data(theme_ques=us_theme_index, final_data=en_ref_data)\n",
    "features.append(en_ref_theme_mean)\n",
    "features.append(zh_ref_theme_mean)\n",
    "feature_name.append(\"Reference US\")\n",
    "feature_name.append(\"Reference CN\")\n",
    "\n",
    "for name in model_name:\n",
    "    cn_score_base = \"D:\\\\projects\\\\LLMValueInvestigation\\\\code\\\\human_evaluation\\\\human_eval\\\\lang_abla\\\\\"\n",
    "    us_score_base = \"D:\\\\projects\\\\LLMValueInvestigation\\\\code\\\\human_evaluation\\\\human_eval\\\\lang_abla\\\\\"\n",
    "    if \"Chinese\" in name:\n",
    "        zh_final = pd.read_csv(cn_score_base + name + \"\\\\final_score.csv\", index_col=0)\n",
    "        zh_theme_mean = prepare_pca_data(theme_ques=cn_theme_index, final_data=zh_final)\n",
    "        features.append(zh_theme_mean)\n",
    "        feature_name.append(f\"{name} CN\")  \n",
    "    elif \"English\" in name:\n",
    "        en_final = pd.read_csv(us_score_base + name + \"\\\\final_score.csv\", index_col=0)\n",
    "        en_theme_mean = prepare_pca_data(theme_ques=us_theme_index, final_data=en_final)\n",
    "        features.append(en_theme_mean)\n",
    "        feature_name.append(f\"{name} US\")  \n",
    "    \n",
    "A = numpy.array(features)\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(A)\n",
    "result = pca.transform(A)\n",
    "\n",
    "pca_datapoint = {\n",
    "    \"Survival v.s. Self-Expression\": result[:,0],\n",
    "    \"Traditional v.s. Secular\": result[:,1],\n",
    "    \"Model type\": feature_name\n",
    "}\n",
    "cord = pd.DataFrame(pca_datapoint, index=feature_name)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2d51d88a-0533-4620-8310-b73481984a32",
   "metadata": {},
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set_context(\"talk\")\n",
    "p1 = sns.scatterplot(\n",
    "    x=cord[\"Survival v.s. Self-Expression\"], \n",
    "    y=cord[\"Traditional v.s. Secular\"], \n",
    "    data=cord, hue=cord[\"Model type\"], \n",
    "    palette ='Paired',sizes=[100]\n",
    ")\n",
    "sns.move_legend(p1, \"upper left\", bbox_to_anchor=(1, 1))\n",
    "# for line in range(0,cord.shape[0]):\n",
    "#      p1.text(cord[\"Survival v.s. Self-Expression\"][line], cord[\"Traditional v.s. Secular\"][line]+0.1, cord.index[line], horizontalalignment='left', size='medium', color='black')\n",
    "dot_fig = p1.get_figure()\n",
    "dot_fig.savefig(\"culmap.pdf\", bbox_inches='tight')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "01784f58-9656-4f7e-ac66-260c5acca88f",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Insensitivity Measurement radar\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import pandas as pd\n",
    "\n",
    "model_name = [\"Mixtral-8x7B-Instruct\"]\n",
    "base = \"F:\\\\Projects\\\\LLMValueInvestigation\\\\code\\\\human_evaluation\\\\gpt_judge\\\\\"\n",
    "\n",
    "for name in model_name:\n",
    "    sns.set()\n",
    "    plt.style.use('ggplot')\n",
    "    data = pd.read_csv(f\"{base}US_survey_all\\\\{name}\\\\AECE_score.csv\", index_col=0)\n",
    "    data = data.sum(axis=0)\n",
    "    total_num = data.sum()\n",
    "    \n",
    "    zh_data = pd.read_csv(f\"{base}CN_survey_all\\\\{name}\\\\AECE_score.csv\", index_col=0)\n",
    "    zh_data = zh_data.sum(axis=0)\n",
    "    zh_total_num = zh_data.sum()\n",
    "    \n",
    "    suffi_list = [i for i in data[1:7]]\n",
    "    zh_suffi_list = [i for i in zh_data[1:7]]\n",
    "    \n",
    "    # for i in data[1:7]:\n",
    "    #     suffi = total_num - i\n",
    "    #     suffi_list.append(suffi)\n",
    "    \n",
    "    labels = np.array([\"IU\", \"QC\", \"RP\", \"CV\", \"FF\", \"IR\"])\n",
    "    # stats = [100.0, 95.0, 95.8, 62.8, 96.8]\n",
    "    \n",
    "    angles = np.linspace(0, 2*np.pi, len(labels), endpoint=False)\n",
    "    stats = np.concatenate((suffi_list, [suffi_list[0]]))\n",
    "    zh_stats = np.concatenate((zh_suffi_list, [zh_suffi_list[0]]))\n",
    "    \n",
    "    angles = np.concatenate((angles, [angles[0]]))\n",
    "    labels = np.concatenate((labels, [labels[0]]))\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, polar=True)\n",
    "    ax.plot(angles, zh_stats, '.-', linewidth=1, label=\"CN\")\n",
    "    ax.plot(angles, stats, '.-', linewidth=1, label=\"US\")\n",
    "    \n",
    "    ax.fill(angles, zh_stats, alpha=0.25)\n",
    "    ax.fill(angles, stats, alpha=0.25)\n",
    "    \n",
    "    # # \n",
    "    # # font = FontProperties(fname=r\"C:\\Windows\\Fonts\\simhei.ttf\", size=14)\n",
    "    ax.set_thetagrids(angles * 180/np.pi, labels)\n",
    "    \n",
    "    # ax.set_ylim(0,5)\n",
    "    plt.legend(loc='upper right', bbox_to_anchor=(1.2,1))\n",
    "    ax.grid(True)\n",
    "    p = ax.get_figure()\n",
    "    p.savefig(f\"{name}-radar.pdf\", bbox_inches='tight')\n",
    "    # plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "408f19cb-232f-4423-b3d4-e05be674cd2d",
   "metadata": {},
   "source": [
    "# Define a function to convert RGB values to a hexadecimal color code\n",
    "def rgb_to_hex(r, g, b):\n",
    "    # Use string formatting to convert the RGB values to a hexadecimal color code\n",
    "    return ('{:02X}' * 3).format(r, g, b)\n",
    "cs = sns.color_palette(\"tab20c\", 11)\n",
    "print(cs.as_hex())\n",
    "cs.as_hex()\n",
    "# for line in cs:\n",
    "#     print(line[:-1])\n",
    "#     co_hex = rgb_to_hex(line[0], line[1], line[2])\n",
    "#     print(co_hex)\n",
    "#     print(a)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f83439b4-ba56-4a8e-92a9-8c4b828013c1",
   "metadata": {},
   "source": [
    "# Insensitivity Measurement radar\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "model_name = [\"Baichuan2-13B-Chat\",\"WizardLM-13B\",\"ChatGLM2-6B\", \"Mistral-7B-Instruct\",\"Dolphin-2.2.1-Mistral-7B\",\"Mixtral-8x7B-Instruct\", \"Llama-3-8B\",\n",
    "              \"Llama-3-8B-Instruct\",\"Dolphin-2.9.1-Llama-3-8B\", \"Llama-3-Chinese-8B-Instruct\", \"Claude-3.5-Sonnet\"]\n",
    "\n",
    "base = \"F:\\\\Projects\\\\LLMValueInvestigation\\\\code\\\\human_evaluation\\\\gpt_judge\\\\\"\n",
    "test_type = \"US_survey_all\"\n",
    "\n",
    "line_style = ['-', '--', ':', '.-', '.-', '.-', '-*', '-*', '-*', '-*', 'o-']\n",
    "cs = sns.color_palette(\"tab20c\", 11)\n",
    "cs_list = cs.as_hex()\n",
    "datas = {}\n",
    "for i in range(len(model_name)):\n",
    "    name = model_name[i]\n",
    "    bai_data = pd.read_csv(f\"{base}{test_type}\\\\{name}\\\\AECE_score.csv\", index_col=0)\n",
    "    bai_data = bai_data.sum(axis=0)\n",
    "    bai_suffi_list = [i for i in bai_data[1:7]]\n",
    "    bai_stats = np.concatenate((bai_suffi_list, [bai_suffi_list[0]]))\n",
    "    datas[name] = (bai_stats, line_style[i], cs_list[i])\n",
    "    \n",
    "plt.style.use('ggplot')\n",
    "\n",
    "labels = np.array([\"IU\", \"QC\", \"RP\", \"CV\", \"FF\", \"IR\"])\n",
    "angles = np.linspace(0, 2*np.pi, len(labels), endpoint=False)\n",
    "angles = np.concatenate((angles, [angles[0]]))\n",
    "labels = np.concatenate((labels, [labels[0]]))\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, polar=True)\n",
    "\n",
    "for model in datas.keys():\n",
    "    ax.plot(angles, datas[model][0], datas[model][1], linewidth=1, label=model, color=datas[model][2])\n",
    "    ax.fill(angles, datas[model][0], alpha=0.25, color=datas[model][2])\n",
    "    # ax.grid(True)\n",
    "\n",
    "ax.set_thetagrids(angles * 180/np.pi, labels)\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1.2, 0.8))\n",
    "ax.grid(True)\n",
    "p = ax.get_figure()\n",
    "p.savefig(f\"{test_type}-radar.pdf\", bbox_inches='tight')\n",
    "# plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c6735750-58f0-49f1-ace6-571ea9ba09a7",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "test_types = [\"CN_survey_all\", \"US_survey_all\"]\n",
    "model_names = [\"baichuan2-13B-4bits\", \"ChatGLM2-6B\", \"dolphin-2.2.1-mistral-7b\", \"dolphin-2.9.1-llama-8b\", \"Llama3-Chinese-8B-Instruct\", \n",
    "               \"Meta-Llama-3-8B\", \"Meta-Llama-3-8B-Instruct\", \"Mistral-7B-Instruct-v0.1\", \"WizardLM-13B-V1.2\"]\n",
    "for test_type in test_types:\n",
    "    sampled_sum = pd.Series([0]*13, index=[str(i) for i in range(0,13)])\n",
    "    for name in model_names:\n",
    "        data_path = rf\"F:\\Projects\\LLMValueInvestigation\\code\\human_evaluation\\gpt_judge\\{test_type}\\{name}\\AECE_score.csv\"\n",
    "        data = pd.read_csv(data_path, index_col=0).drop(0)\n",
    "        cleaned_data = data.loc[(data != 0).any(axis=1)]\n",
    "        rand_index = random.sample(sorted(cleaned_data.index), 10)\n",
    "        sampled_data = []\n",
    "        for ind in rand_index:\n",
    "            sampled_data.append(cleaned_data.loc[ind])\n",
    "        sample_data = pd.concat(sampled_data, ignore_index=True, axis=1)\n",
    "        sampled_sum = sampled_sum.radd(sample_data.sum(axis=1))\n",
    "    sampled_sum.to_csv(f\"{test_type}_sampled_sum.csv\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "026fe74a-77c5-4220-93ee-01e0af652d0f",
   "metadata": {},
   "source": [
    "import seaborn as sns\n",
    "sns.set_context(\"paper\")\n",
    "sns.set_style(\"white\")\n",
    "cn_model = [27,20,31,34,47,34, 419]\n",
    "cn_human = [45,24,33,48,43,59, 410]\n",
    "us_model = [30,34,30,46,56,41, 387]\n",
    "us_human = [55,43,49,59,50,68, 360]\n",
    "cn_data = {\n",
    "    \"Insensitivity Measurement Dimensions\": [\"UI\", \"QC\", \"RP\", \"CV\", \"FF\", \"IR\", \"Pass\"]*2,\n",
    "    \"Score\": cn_model + cn_human,\n",
    "    \"From\": [\"GPT-3.5\"]*len(cn_model) + [\"Human\"]*len(cn_human),\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "fig = sns.lineplot(cn_data, x=\"Insensitivity Measurement Dimensions\", y=\"Score\", hue=\"From\", palette=\"YlOrBr\")\n",
    "p = fig.get_figure()\n",
    "p.savefig(\"CN_judge_effect.pdf\", bbox_inches='tight')\n",
    "\n",
    "# us_data = {\n",
    "#     \"Insensitivity Measurement Dimensions\": [\"UI\", \"QC\", \"RP\", \"CV\", \"FF\", \"IR\", \"Pass\"]*2,\n",
    "#     \"Score\": us_model + us_human,\n",
    "#     \"From\": [\"GPT-3.5\"]*len(cn_model) + [\"Human\"]*len(cn_human),\n",
    "# }\n",
    "# df = pd.DataFrame(data)\n",
    "# fig = sns.lineplot(us_data, x=\"Insensitivity Measurement Dimensions\", y=\"Score\", hue=\"From\", palette=\"Blues\")\n",
    "# p = fig.get_figure()\n",
    "# p.savefig(\"US_judge_effect.pdf\", bbox_inches='tight')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ecb863d5-bda0-4216-aef2-020877d85725",
   "metadata": {},
   "source": [
    "import json\n",
    "from typing import Dict, List\n",
    "import pandas as pd\n",
    "\n",
    "data_json_path = r\"F:\\Projects\\EMNLP\\llama3-main\\llama3-main\\data\\sft_wvs_train.json\"\n",
    "with open(data_json_path, \"r\", encoding=\"utf-8\")as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "country_list = []\n",
    "for d in data:\n",
    "    instruct = d[\"instruction\"]\n",
    "    country_start = instruct.find(\"How would someone from \") + len(\"How would someone from \")\n",
    "    country_end = instruct.find(\" answer the following question:\")\n",
    "    country = instruct[country_start: country_end]\n",
    "    country_list.append(country)\n",
    "    \n",
    "dis_data: Dict[str, List] = {k: [] for k in country_list}\n",
    "for d in data:\n",
    "    instruct = d[\"instruction\"]\n",
    "    country_start = instruct.find(\"How would someone from \") + len(\"How would someone from \")\n",
    "    country_end = instruct.find(\" answer the following question:\")\n",
    "    country = instruct[country_start: country_end]\n",
    "    dist_list = list(d[\"options_dist\"].values())\n",
    "    dis_data[country].append(dist_list)\n",
    "\n",
    "c_datas = []\n",
    "for c in country_list:\n",
    "    maxlen = max(len(d_line) for d_line in dis_data[c])\n",
    "    for line in dis_data[c]:\n",
    "        if len(line) < maxlen:\n",
    "            line += [0]*(maxlen-len(line))\n",
    "    c_data_dict = {str(i): dis_data[c][i] for i in range(len(dis_data[c]))}\n",
    "    c_data = pd.DataFrame(c_data_dict).T\n",
    "    c_datas.append(c_data)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3ec58364-8249-4612-9e00-165059020685",
   "metadata": {},
   "source": [
    "def prepare_pca_data(theme_ques, final_data):\n",
    "    theme_mean_list = []\n",
    "    for theme in theme_ques:\n",
    "        theme_sum = 0\n",
    "        theme_count = 0\n",
    "        for ques in theme:\n",
    "            if ques < len(final_data):\n",
    "                ques = str(ques)\n",
    "                if ques not in [7,8] and final_data.loc[ques].sum() != 0:\n",
    "                    q_mean = 0\n",
    "                    for choice in final_data.loc[ques].index:\n",
    "                        if choice != \"12\":\n",
    "                            q_mean += float(choice) * (final_data.loc[ques][choice] / final_data.loc[ques].sum())\n",
    "                    theme_sum += q_mean\n",
    "                    theme_count+=1\n",
    "        theme_mean = theme_sum / theme_count\n",
    "        theme_mean_list.append(theme_mean)\n",
    "    return theme_mean_list"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "eee733ef-d3b8-4320-98f3-bdad80e87f9a",
   "metadata": {},
   "source": [
    "us_question_type = pd.read_excel(\n",
    "    \"F:\\\\Projects\\\\LLMValueInvestigation\\\\code\\\\human_evaluation\\\\survey_data\\\\AECE_v1\\\\Originals\\\\US_survey\\\\result_database\\\\Question_type.xlsx\")\n",
    "type_index = us_question_type.groupby(\"value type\")\n",
    "us_theme_index = []\n",
    "for index, value in type_index:\n",
    "    us_theme_index.append(value[\"id\"].values.tolist())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ebc5573a-1465-4857-95af-029eb286091f",
   "metadata": {},
   "source": [
    "import numpy\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "features = []\n",
    "for c_data in c_datas:\n",
    "    features.append(prepare_pca_data(us_theme_index, c_data))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f49eb016-7f0d-46de-98a0-b576e64e5021",
   "metadata": {},
   "source": [
    "A = numpy.array(features)\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(A)\n",
    "result = pca.transform(A)\n",
    "\n",
    "pca_datapoint = {\n",
    "    \"Survival v.s. Self-Expression\": result[:,0],\n",
    "    \"Traditional v.s. Secular\": result[:,1],\n",
    "    \"Country\": country_list\n",
    "}\n",
    "cord = pd.DataFrame(pca_datapoint, index=country_list).drop_duplicates()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "420efda6-3fe0-4975-989a-cf8cec86b581",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set_context(\"notebook\")\n",
    "p1 = sns.scatterplot(\n",
    "    x=cord[\"Survival v.s. Self-Expression\"], \n",
    "    y=cord[\"Traditional v.s. Secular\"], \n",
    "    data=cord, hue=cord[\"Country\"], \n",
    "    palette ='tab20',sizes=[100]\n",
    ")\n",
    "for line in range(0,cord.shape[0]):\n",
    "     p1.text(\n",
    "         cord[\"Survival v.s. Self-Expression\"][line], \n",
    "         cord[\"Traditional v.s. Secular\"][line]-0.01, \n",
    "         cord.index[line], \n",
    "         horizontalalignment='left', size='small', color='black'\n",
    "     )\n",
    "plt.legend(ncols=3, loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
    "# sns.move_legend(p1, \"upper left\", bbox_to_anchor=(1, 1))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0665668a-854d-41a5-be3d-a4a00f8bbbc5",
   "metadata": {},
   "source": [
    "import seaborn as sns\n",
    "# filtered_cord = pd.DataFrame(column=cord.column, index=[])\n",
    "filter = []\n",
    "for i in cord.index:\n",
    "    c = cord.loc[i]\n",
    "    if c[\"Survival v.s. Self-Expression\"] >= 0 and c[\"Traditional v.s. Secular\"] < 0:\n",
    "        filter.append(c)\n",
    "filtered_cord = pd.concat(filter, axis=1)\n",
    "filtered_data = pd.DataFrame(filtered_cord).T\n",
    "filtered_data\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "af4a08bd-127d-4a5c-b13c-44c289167314",
   "metadata": {},
   "source": [
    "sns.set_context(\"notebook\")\n",
    "p1 = sns.scatterplot(\n",
    "    x=filtered_data[\"Survival v.s. Self-Expression\"], \n",
    "    y=filtered_data[\"Traditional v.s. Secular\"], \n",
    "    data=filtered_data, hue=filtered_data[\"Country\"], \n",
    "    palette ='tab20',sizes=[100]\n",
    ")\n",
    "for line in range(0,filtered_data.shape[0]):\n",
    "     p1.text(\n",
    "         filtered_data[\"Survival v.s. Self-Expression\"][line], \n",
    "         filtered_data[\"Traditional v.s. Secular\"][line]-0.01, \n",
    "         filtered_data.index[line], \n",
    "         horizontalalignment='left', size='small', color='black'\n",
    "     )\n",
    "sns.move_legend(p1, \"upper left\", bbox_to_anchor=(1, 1))"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
